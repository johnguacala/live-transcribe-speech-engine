#!/usr/bin/env python3
"""
ğŸ¤ MÃ³dulo de DiarizaciÃ³n con WhisperX
Sistema avanzado para transcripciÃ³n con identificaciÃ³n de speakers

Autor: John Guarenas
Proyecto: HearingsWhisper
"""

import whisperx
import torch
from pathlib import Path
import time
from typing import List, Dict, Tuple
import json
from datetime import datetime, timedelta
import re

from logger import TranscriptionLogger

class WhisperXDiarizer:
    """Transcriptor con diarizaciÃ³n usando WhisperX"""
    
    def __init__(self, logger: TranscriptionLogger):
        self.logger = logger
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.compute_type = "float16" if torch.cuda.is_available() else "int8"
        
        self.logger.info(f"ğŸ”§ Inicializando WhisperX en dispositivo: {self.device}")
        
        # Modelos que se cargarÃ¡n bajo demanda
        self.asr_model = None
        self.diarize_model = None
        self.align_model = None
        self.metadata = None
    
    def load_models(self):
        """Carga todos los modelos necesarios"""
        try:
            self.logger.info("ğŸ“¥ Cargando modelo ASR (Whisper)...")
            self.asr_model = whisperx.load_model("large-v2", self.device, compute_type=self.compute_type)
            
            self.logger.info("ğŸ“¥ Cargando modelo de diarizaciÃ³n...")
            self.diarize_model = whisperx.DiarizationPipeline(use_auth_token=None, device=self.device)
            
            self.logger.info("âœ… Modelos cargados exitosamente")
            
        except Exception as e:
            self.logger.error(f"Error cargando modelos WhisperX: {e}")
            raise
    
    def transcribe_with_diarization(self, audio_path: Path, language: str = "es") -> Dict:
        """
        Transcribe audio con diarizaciÃ³n completa
        
        Args:
            audio_path: Path al archivo de audio
            language: Idioma del audio
        
        Returns:
            Diccionario con transcripciÃ³n y informaciÃ³n de speakers
        """
        if not self.asr_model:
            self.load_models()
        
        self.logger.info(f"ğŸ¤ Iniciando transcripciÃ³n con diarizaciÃ³n: {audio_path.name}")
        
        try:
            # 1. TranscripciÃ³n inicial
            self.logger.info("ğŸ“ Paso 1: Transcribiendo audio...")
            audio = whisperx.load_audio(str(audio_path))
            result = self.asr_model.transcribe(audio, batch_size=16)
            
            # 2. AlineaciÃ³n (mejora la precisiÃ³n de los timestamps)
            self.logger.info("â±ï¸ Paso 2: Alineando timestamps...")
            model_a, metadata = whisperx.load_align_model(language_code=language, device=self.device)
            result = whisperx.align(result["segments"], model_a, metadata, audio, self.device, return_char_alignments=False)
            
            # 3. DiarizaciÃ³n (identificaciÃ³n de speakers)
            self.logger.info("ğŸ‘¥ Paso 3: Identificando speakers...")
            diarize_segments = self.diarize_model(audio)
            result = whisperx.assign_word_speakers(diarize_segments, result)
            
            self.logger.info(f"âœ… TranscripciÃ³n completada con {len(result['segments'])} segmentos")
            
            return {
                'segments': result['segments'],
                'language': language,
                'audio_path': str(audio_path),
                'processing_time': time.time(),
                'device_used': self.device
            }
            
        except Exception as e:
            self.logger.error(f"Error en transcripciÃ³n con diarizaciÃ³n: {e}")
            raise
    
    def format_timecode(self, seconds: float) -> str:
        """
        Convierte segundos a formato timecode HH;MM;SS;FF
        
        Args:
            seconds: Tiempo en segundos
            
        Returns:
            String en formato 00;00;00;00
        """
        # Convertir a horas, minutos, segundos y frames (asumiendo 25 fps)
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        frames = int((seconds % 1) * 25)  # 25 frames por segundo
        
        return f"{hours:02d};{minutes:02d};{secs:02d};{frames:02d}"
    
    def process_segments_to_formatted_text(self, transcription_result: Dict, speaker_mapping: Dict = None) -> str:
        """
        Convierte los segmentos a formato de texto personalizado
        
        Args:
            transcription_result: Resultado de la transcripciÃ³n
            speaker_mapping: Mapeo de speakers (ej: {"SPEAKER_00": "Juez", "SPEAKER_01": "Fiscal"})
            
        Returns:
            Texto formateado con timecodes y speakers
        """
        segments = transcription_result['segments']
        formatted_lines = []
        
        # Header con informaciÃ³n del archivo
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        audio_path = Path(transcription_result['audio_path']).name
        
        header = f"""# TranscripciÃ³n con DiarizaciÃ³n - {audio_path}
# Fecha: {timestamp}
# Modelo: WhisperX large-v2
# Idioma: {transcription_result['language']}
# Dispositivo: {transcription_result['device_used']}

"""
        formatted_lines.append(header)
        
        current_speaker = None
        current_text_parts = []
        current_start_time = None
        
        for segment in segments:
            speaker = segment.get('speaker', 'SPEAKER_UNKNOWN')
            text = segment.get('text', '').strip()
            start_time = segment.get('start', 0)
            
            # Aplicar mapeo de speakers si existe
            if speaker_mapping and speaker in speaker_mapping:
                display_speaker = speaker_mapping[speaker]
            else:
                display_speaker = speaker
            
            # Si cambia el speaker o es el primer segmento
            if speaker != current_speaker:
                # Guardar el segmento anterior si existe
                if current_speaker is not None and current_text_parts:
                    timecode = self.format_timecode(current_start_time)
                    combined_text = ' '.join(current_text_parts).strip()
                    
                    formatted_lines.append(f"{timecode}")
                    formatted_lines.append(f"{current_display_speaker}")
                    formatted_lines.append(f"{combined_text}")
                    formatted_lines.append("")  # LÃ­nea en blanco
                
                # Iniciar nuevo segmento
                current_speaker = speaker
                current_display_speaker = display_speaker
                current_text_parts = [text] if text else []
                current_start_time = start_time
            else:
                # Continuar con el mismo speaker
                if text:
                    current_text_parts.append(text)
        
        # Agregar el Ãºltimo segmento
        if current_speaker is not None and current_text_parts:
            timecode = self.format_timecode(current_start_time)
            combined_text = ' '.join(current_text_parts).strip()
            
            formatted_lines.append(f"{timecode}")
            formatted_lines.append(f"{current_display_speaker}")
            formatted_lines.append(f"{combined_text}")
        
        return '\n'.join(formatted_lines)
    
    def get_speakers_summary(self, transcription_result: Dict) -> Dict:
        """
        Obtiene un resumen de los speakers detectados
        
        Args:
            transcription_result: Resultado de la transcripciÃ³n
            
        Returns:
            Diccionario con informaciÃ³n de speakers
        """
        segments = transcription_result['segments']
        speakers = {}
        
        for segment in segments:
            speaker = segment.get('speaker', 'SPEAKER_UNKNOWN')
            text = segment.get('text', '').strip()
            duration = segment.get('end', 0) - segment.get('start', 0)
            
            if speaker not in speakers:
                speakers[speaker] = {
                    'total_duration': 0,
                    'segment_count': 0,
                    'first_words': text[:50] if text else '',
                    'total_words': 0
                }
            
            speakers[speaker]['total_duration'] += duration
            speakers[speaker]['segment_count'] += 1
            speakers[speaker]['total_words'] += len(text.split()) if text else 0
        
        return speakers
    
    def create_speaker_mapping_prompt(self, speakers_summary: Dict) -> str:
        """
        Crea un prompt para que el usuario asigne nombres a los speakers
        
        Args:
            speakers_summary: Resumen de speakers detectados
            
        Returns:
            String con informaciÃ³n para asignar nombres
        """
        prompt_lines = [
            "ğŸ¤ SPEAKERS DETECTADOS:",
            "=" * 50,
            ""
        ]
        
        for speaker, info in speakers_summary.items():
            duration_mins = info['total_duration'] / 60
            prompt_lines.extend([
                f"Speaker: {speaker}",
                f"  â€¢ DuraciÃ³n total: {duration_mins:.1f} minutos",
                f"  â€¢ Segmentos: {info['segment_count']}",
                f"  â€¢ Palabras totales: {info['total_words']}",
                f"  â€¢ Primeras palabras: \"{info['first_words']}...\"",
                ""
            ])
        
        prompt_lines.extend([
            "ğŸ“ ASIGNACIÃ“N DE NOMBRES:",
            "Para asignar nombres reales a los speakers, crea un archivo llamado",
            "'speaker_mapping.json' con el siguiente formato:",
            "",
            "{",
            '  "SPEAKER_00": "Juez",',
            '  "SPEAKER_01": "Fiscal",', 
            '  "SPEAKER_02": "LCDO"',
            "}",
            "",
            "El sistema aplicarÃ¡ automÃ¡ticamente estos nombres en la transcripciÃ³n final."
        ])
        
        return '\n'.join(prompt_lines)
    
    def save_transcription(self, transcription_result: Dict, output_path: Path, speaker_mapping: Dict = None) -> Path:
        """
        Guarda la transcripciÃ³n en formato de texto personalizado
        
        Args:
            transcription_result: Resultado de la transcripciÃ³n
            output_path: Path donde guardar el archivo
            speaker_mapping: Mapeo opcional de speakers
            
        Returns:
            Path del archivo guardado
        """
        formatted_text = self.process_segments_to_formatted_text(transcription_result, speaker_mapping)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(formatted_text)
        
        self.logger.info(f"ğŸ’¾ TranscripciÃ³n guardada en: {output_path}")
        return output_path
